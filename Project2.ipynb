{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # Used for the conversion of \"r##_c##\" in only the numbers --> TODO: check where it comes from\n",
    "from IPython.display import display\n",
    "from helpers import *\n",
    "from play_with_data import *\n",
    "from pre_processing import *\n",
    "from matrix_factorization import *\n",
    "from cross_validation import *\n",
    "from apply_classifiers import *\n",
    "from trainings_submissions import *\n",
    "from regressions_models import *\n",
    "from majority_mean import *\n",
    "import scipy.sparse as sp # In order to use sparse \n",
    "# Predictors imported in performance order (best to worst, according to http://surpriselib.com/)\n",
    "from surprise import SVDpp\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import SlopeOne\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF\n",
    "from surprise import CoClustering\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithZScore # not scored --> to be tested quickly\n",
    "from surprise import dataset\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise import GridSearch\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Creation of a sparse matrix of the data (training set)**********\n",
    "ratings = load_data('./data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Creatuib of a sparse matrix of the data (test set) ********\n",
    "test = load_data('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of the given data and some statistics\n",
    "- We load the training data with another method in order to do some statistics\n",
    "- All we do here is in order to learn more about the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Load the given data **********\n",
    "r_c, x = load_data_old('./data_train.csv') #r_c contains the position (userID_movieID) and x contains the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Creation of a matrix of the data ********\n",
    "nUser = 10000 # These numbers were given\n",
    "nItem = 1000 # These numbers were given\n",
    "data = np.zeros([nUser, nItem]) # Initialization of the matrix\n",
    "\n",
    "for ind, i in enumerate(r_c): # Loop over all the IDs, in order to create a numpy matrix\n",
    "    data[int(re.findall('\\d+', i)[0])-1, int(re.findall('\\d+', i)[1])-1] = x[ind] # Use the information in the ID (row, col) to create the matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********** Data preview ************\n",
    "# Check if there is any missed data \n",
    "# It was told us that we have the data from 10'000 users for 1'000 films, but we don't have all these ratings\n",
    "info_general(nUser, nItem, x, data) # Call of a method that will print some general information about the data\n",
    "print('\\n')\n",
    "info_ratings(data) # Call a method that will print some information about the ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this barplot, we can see that ratings are not distributed in an uniform way, this may suggest that there is a bias in the rating matrix that has to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***** Data preview (cont'd) *********\n",
    "# Information about the number of ratings for the users and for the movies\n",
    "num_movies_per_user, num_users_per_movie = plot_raw_data(ratings) # Original code is from the course, ex10 'plots.py'\n",
    "print(\"Maximum number of movies per user:\\t{}\\nMinimum number of movies per user:\\t{}\\n\".format(np.max(num_movies_per_user), np.min(num_movies_per_user)))\n",
    "print(\"Maximum number of users per movie:\\t{}\\nMinimum number of users per movie:\\t{}\".format(np.max(num_users_per_movie), np.min(num_users_per_movie)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms from \"Surprise\"\n",
    "- First, cross validation on the training set in order to have an idea of the performance of each algorithms WITHOUT any optimization\n",
    "- Then we optimized the algorithms, this can be seen in the notebook: algorithm_optimization.ipynb\n",
    "- After that we do again the cross validation on the training set in order to show the performance of each optimized algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation to evaluate the performance of the algorithms (without any optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********** Formating the data correctly for Surprise + Cross Validation *************\n",
    "ratings_surpr = formating_data_surprise(ratings) # Call a method that will transform the ratings in the right format\n",
    "ratings_surpr.split(n_folds=3) # Will create the 3 folds for cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Evaluate all the algorithms ########################\n",
    "algos = [SVDpp(),KNNBaseline(),NMF(),SVD(),SlopeOne(),BaselineOnly(),KNNWithZScore()]\n",
    "perf = {}\n",
    "algo_str = ['SVDpp','KNNBaseline','NMF','SVD','SlopeOne','BaselineOnly','KNNWithZScore']\n",
    "\n",
    "for i,algo in enumerate(algos): # Loop over the algorithms \n",
    "    # Evaluate performances of \"Surprise\" algorithm on the dataset\n",
    "    perf[algo_str[i]] = evaluate(algo, ratings_surpr, measures=['RMSE']) # Evaluate the performance of each algo by cross validation \n",
    "    print_perf(perf[algo_str[i]]) # Print the performance for each algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- SAVE -----------------\n",
    "# Uncomment the line just below if you want to save the variable\n",
    "#np.save('perf_dictionary.npy', perf) # Saving the dictionary that contains the RMSE of all the algos evaluated above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of the algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- Grid search and the optimisation is in the Jupyter Notebook: algorithm_optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation to evaluate the performance of the algorithms (with optimization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Evaluate all the algorithms ########################\n",
    "algos = [SVDpp(n_factors=10,lr_all=0.00177827941004,reg_all=0.001),\n",
    "         KNNBaseline(k=96, min_k=8,sim_options={'name': 'pearson_baseline','user_based': False,'shrinkage': 500},bsl_options={'method': 'als','reg_u': 14.4,'reg_i': 0.3}),\n",
    "         NMF(n_factors=35,reg_pu=10**(-1.5),reg_qi=10**(-0.5)),\n",
    "         SVD(n_factors=100,lr_all=0.001,reg_all=10**(-1.5)),\n",
    "         SlopeOne(),\n",
    "         BaselineOnly(bsl_options={'method': 'als', 'reg_u': 14.4, 'reg_i': 0.3}),\n",
    "         KNNWithZScore(k=100, min_k=7, sim_options={'name':'pearson_baseline','user_based':False,'shrinkage':500})\n",
    "        ]\n",
    "perf = {}\n",
    "algo_str = ['SVDpp_optimized','KNNBaseline_optimized','NMF_optimized','SVD_optimized','SlopeOne_optimized',\n",
    "            'BaselineOnly_optimized','KNNWithZScore_optimized']\n",
    "\n",
    "for i,algo in enumerate(algos): # Loop over the algorithms \n",
    "    # Evaluate performances of \"Surprise\" algorithm on the dataset\n",
    "    perf[algo_str[i]] = evaluate(algo, ratings_surpr, measures=['RMSE']) # Evaluate the performance of each algo by cross validation \n",
    "    print_perf(perf[algo_str[i]]) # Print the performance for each algo \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use algorithms' predictions to find the best predicted ratings\n",
    "\n",
    "First we set up all the variables, algorithms, regressors and classifiers\n",
    "Then we apply these different strategies:\n",
    "1. Majority vote\n",
    "2. Mean of the predictions\n",
    "3. Apply classifiers on the predictions (from sklearn)\n",
    "4. Apply regressors on the predictions (from sklearn and XGBoost)\n",
    "\n",
    "Then we will choose the best one of the four cited just above and try to optimize some parameters to have an even better prediction\n",
    "\n",
    "What we will do in details:\n",
    "- Define the algorithms with the best parameters (find with the optimization done in \"algorithm_optimization.ipynb\")\n",
    "- Create a training set (70%) and a validation set (30%) from the \"data_train.csv\"\n",
    "- Train the algorithms on the training set\n",
    "- Apply the trained algorithms on the validation set in order to have the predictions for each algorithms\n",
    "- Apply the different strategies quoted above in order to have the final prediction\n",
    "    - The strategies are explained below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up everything (algorithms, regressors and classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Algos with optimized parameters ##############\n",
    "\n",
    "#***** All the algorithms we are using (optimized) *******\n",
    "algos = [SVDpp(n_factors=10,lr_all=0.00177827941004,reg_all=0.001),\n",
    "         KNNBaseline(k=96, min_k=8,sim_options={'name': 'pearson_baseline','user_based': False,'shrinkage': 500},bsl_options={'method': 'als','reg_u': 14.4,'reg_i': 0.3}),\n",
    "         NMF(n_factors=35,reg_pu=10**(-1.5),reg_qi=10**(-0.5)),\n",
    "         SVD(n_factors=100,lr_all=0.001,reg_all=10**(-1.5)),\n",
    "         SlopeOne(),\n",
    "         BaselineOnly(bsl_options={'method': 'als', 'reg_u': 14.4, 'reg_i': 0.3}),\n",
    "         KNNWithZScore(k=100, min_k=7, sim_options={'name':'pearson_baseline','user_based':False,'shrinkage':500})\n",
    "        ]\n",
    "\n",
    "\n",
    "############ Define some variables #################\n",
    "columns_name = ['SVDpp','KNNBaseline','NMF','SVD','SlopeOne','BaselineOnly','KNNWithZScore'] # Name of the algorithms, useful for the dataframe\n",
    "\n",
    "#*********** Regressors ***************\n",
    "regressions_method = [XGBRegressor(),\n",
    "                      linear_model.LinearRegression(), \n",
    "                      linear_model.Ridge(), \n",
    "                      linear_model.Lasso(), \n",
    "                      linear_model.BayesianRidge(),\n",
    "                      linear_model.ElasticNet(),\n",
    "                      linear_model.HuberRegressor(),\n",
    "                      linear_model.LassoLars(),\n",
    "                      linear_model.PassiveAggressiveRegressor(),\n",
    "                      linear_model.SGDRegressor()\n",
    "                     ]\n",
    "\n",
    "#*********** Classifiers ***************\n",
    "classifiers_method = [naive_bayes,\n",
    "                      kNearestNeigh, \n",
    "                      decision_tree, \n",
    "                      neural_net, \n",
    "                      support_vectorMachine, \n",
    "                      discr_analysis, \n",
    "                      lin_discr_analysis\n",
    "                     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= TRAINING ====================\n",
    "#********* Creation of train and validation ratings *********\n",
    "train_ratings, validation_ratings = split_data(ratings, prob_test=0.3) # splitting in train and validation set\n",
    "\n",
    "#********* Do the training with all the algos ****************\n",
    "print('-----START -----\\nTraining of the algos\\n')\n",
    "algos_trained = first_train(train_ratings,algos) # Call function first_train --> train the algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Validation set up ============\n",
    "#******** prepare the validation set *********\n",
    "validation_df, validation_surprise = formating_data_surprise(validation_ratings, True) # Formating the data in order to use Surprise\n",
    "validation_set = validation_surprise.build_full_trainset() # Build trainset\n",
    "validation_set_pred = validation_set.build_testset() # Build iterable object in order to test \n",
    "\n",
    "prediction_validation_df = validation_df.copy() # Copy in order to keep the original intact \n",
    "\n",
    "######### Predictions by the trained algorithms #############\n",
    "for i, algo_t in enumerate(algos_trained): # Loop over all the trained algorithms\n",
    "    pred = algo_t.test(validation_set_pred) # Make the prediction\n",
    "\n",
    "    ########## Creation of the list: estim ########\n",
    "    estim = [] # initialization of the list estim\n",
    "\n",
    "    for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "        estim.append(p.est) # fill this list with the ratings\n",
    "\n",
    "    d = {'prediction' : pd.Series(estim)}\n",
    "    temp = pd.DataFrame(d)\n",
    "    prediction_validation_df = pd.concat([prediction_validation_df,temp], axis=1) # Add all the predictions\n",
    "\n",
    "first_col = ['movies ID', 'Label', 'users ID'] # Name of the first columns\n",
    "all_col = first_col + columns_name # All the columns name\n",
    "prediction_validation_df.columns = all_col # Put the names of the column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the data we get after the predictions on the validation set')\n",
    "display(prediction_validation_df.head())\n",
    "print('\\nThis is the data we get after the predictions on the validation set and after just removed \"movies ID\" and \"users ID\"')\n",
    "prediction_label_df = prediction_validation_df.copy()\n",
    "prediction_label_df = prediction_label_df.drop(['movies ID', 'users ID'], axis = 1)\n",
    "display(prediction_label_df.head())\n",
    "\n",
    "#----------- SAVE -----------------\n",
    "# Uncomment thes lines just below if you want to save the variable\n",
    "\n",
    "#prediction_validation_df.to_csv('prediction_df_validation.csv')\n",
    "#np.save('algos_trained_training', algos_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Majority vote\n",
    "- We use the algorithms trained on the training set to create predictions on the validation set. Then we apply the majority vote algorithm in order to obtain a final prediction. We also compute the rmse with the real ratings.\n",
    "\n",
    "This strategy is quite simple and will choose the rating that is the most often predicted amongst the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************* Apply majority on the validation set *************\n",
    "validation_pred_majority, validation_rmse_majority = majority_vote(prediction_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE obtained on the validation set with Majority vote: {}'.format(validation_rmse_majority))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean vote\n",
    "- We use the algorithms trained on the training set to create predictions on the validation set. Then we apply the mean vote algorithm in order to obtain a final prediction. We also compute the rmse with the real ratings.\n",
    "\n",
    "This strategy is also quite simple, we take the mean of the ratings across all the algorithms for each user and movie pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************* Apply mean on the validation set *************\n",
    "validation_pred_mean, validation_rmse_mean = mean_vote(prediction_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE obtained on the validation set with Mean vote: {}'.format(validation_rmse_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifiers on predictions\n",
    "- We use the predictions done by the trained (optimized) algorithms and try to apply several different classifiers on them.\n",
    "- We will choose the best classifier based on the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Find the best classifier (with the lowest RMSE) **********\n",
    "print('-----START -----\\nSelection of classifier\\n')\n",
    "\n",
    "clf, best_rmse_clf = apply_classifier(prediction_label_df, classifiers_method) # Call a method that will find the best classifier and its RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Print the best classifier and its RMSE ############\n",
    "print(clf)\n",
    "print('\\nRMSE obtained with the best classifier: {}'.format(best_rmse_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regressors on predictions\n",
    "- We use the predictions done by the trained (optimized) algorithms and try to apply several different regressors on them.\n",
    "- We will choose the best regressor based on the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Find the best classifier (with the lowest RMSE) **********\n",
    "print('-----START -----\\nSelection of regressor\\n')\n",
    "reg, best_rmse_reg = lin_regressors(prediction_label_df, regressions_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Print the best regressor and its RMSE ############\n",
    "print(reg)\n",
    "print('\\nRMSE obtained with the best regressor: {}'.format(best_rmse_reg))\n",
    "print('\\nThe coefficients are: {}'.format(reg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- We can see that the regression technique is the one that works better, but we also see that when we run all the notebook multiple times the best regressor is not always the same. This is because in the split_data method we have a random function, so when we do the splitting there is a random effect that will not always be the same.\n",
    "- So now we will try to improve the best regressor (obtained once) by optimizing the parameters of this regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- We take the best regressor obtained once we run the all notebook and now we will try to find the best parameters for this regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Define the best regressor and some parameters ############\n",
    "regressor_toOptimize = linear_model.SGDRegressor()\n",
    "hyperparams = {'alpha': np.logspace(-6,-2,5) , \n",
    "               'l1_ratio': np.linspace(0,0.6,5),  \n",
    "               'epsilon': np.logspace(-3,1,5)\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Do the Grid Search optimization ###########\n",
    "\n",
    "regressor_optimized = optimization_regressor(prediction_validation_df, regressor_toOptimize, hyperparams)\n",
    "print(regressor_optimized) # will show the best param and also all the possible values of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('These are the best parameters: {}'.format(regressor_optimized.best_params_))\n",
    "\n",
    "#--------- SAVE -----------\n",
    "# Uncomment if you want to save \n",
    "#np.save('best_regressor_param', regressor_optimized.best_params_)\n",
    "#np.save('regressor_optimized', regressor_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model\n",
    "As explained in the report, after trying with quite complex strategies and not succeed to improve our score. We try with something much simpler. We use the optimized algorithm SVDpp trained on the whold training set.\n",
    "\n",
    "You can run only this following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Call our final model \n",
    "from run import *\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
