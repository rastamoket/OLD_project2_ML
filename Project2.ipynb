{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # Used for the conversion of \"r##_c##\" in only the numbers --> TODO: check where it comes from\n",
    "from IPython.display import display\n",
    "from helpers import *\n",
    "from play_with_data import *\n",
    "from pre_processing import *\n",
    "from matrix_factorization import *\n",
    "from cross_validation import *\n",
    "from apply_classifiers import *\n",
    "from trainings_submissions import *\n",
    "from regressions_models import *\n",
    "from majority_mean import *\n",
    "import scipy.sparse as sp # In order to use sparse \n",
    "# Predictors imported in performance order (best to worst, according to http://surpriselib.com/)\n",
    "from surprise import SVDpp\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import SlopeOne\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF\n",
    "from surprise import CoClustering\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithZScore # not scored --> to be tested quickly\n",
    "from surprise import dataset\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise import GridSearch\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Creation of a sparse matrix of the data (training set)**********\n",
    "ratings = load_data('./data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Creatuib of a sparse matrix of the data (test set) ********\n",
    "test = load_data('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of the given data and some statistics\n",
    "- We load the training data with another method in order to do some statistics\n",
    "- All we do here is in order to learn more about the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Load the given data **********\n",
    "r_c, x = load_data_old('./data_train.csv') #r_c contains the position (userID_movieID) and x contains the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Creation of a matrix of the data ********\n",
    "nUser = 10000 # These numbers were given\n",
    "nItem = 1000 # These numbers were given\n",
    "data = np.zeros([nUser, nItem]) # Initialization of the matrix\n",
    "\n",
    "for ind, i in enumerate(r_c): # Loop over all the IDs, in order to create a numpy matrix\n",
    "    data[int(re.findall('\\d+', i)[0])-1, int(re.findall('\\d+', i)[1])-1] = x[ind] # Use the information in the ID (row, col) to create the matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********** Data preview ************\n",
    "# Check if there is any missed data \n",
    "# It was told us that we have the data from 10'000 users for 1'000 films, but we don't have all these ratings\n",
    "info_general(nUser, nItem, x, data) # Call of a method that will print some general information about the data\n",
    "print('\\n')\n",
    "info_ratings(data) # Call a method that will print some information about the ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this barplot, we can see that ratings are not distributed in an uniform way, this may suggest that there is a bias in the rating matrix that has to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***** Data preview (cont'd) *********\n",
    "# Information about the number of ratings for the users and for the movies\n",
    "num_movies_per_user, num_users_per_movie = plot_raw_data(ratings) # Original code is from the course, ex10 'plots.py'\n",
    "print(\"Maximum number of movies per user:\\t{}\\nMinimum number of movies per user:\\t{}\\n\".format(np.max(num_movies_per_user), np.min(num_movies_per_user)))\n",
    "print(\"Maximum number of users per movie:\\t{}\\nMinimum number of users per movie:\\t{}\".format(np.max(num_users_per_movie), np.min(num_users_per_movie)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms from \"Surprise\"\n",
    "- First, cross validation on the training set in order to have an idea of the performance of each algorithms WITHOUT any optimization\n",
    "- Second, trying to optimize the algorithms using Grid Search (from Surprise) --> TODO: Ivan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation to evaluate the performance of the algorithms (without any optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********** Formating the data correctly for Surprise + Cross Validation *************\n",
    "ratings_surpr = formating_data_surprise(ratings) # Call a method that will transform the ratings in the right format\n",
    "ratings_surpr.split(n_folds=3) # Will create the 3 folds for cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Evaluate all the algorithms ########################\n",
    "algos = [SVDpp(),KNNBaseline(),NMF(),SVD(),SlopeOne(),BaselineOnly(),KNNWithZScore()]\n",
    "perf = {}\n",
    "algo_str = ['SVDpp','KNNBaseline','NMF','SVD','SlopeOne','BaselineOnly','KNNWithZScore']\n",
    "\n",
    "for i,algo in enumerate(algos): # Loop over the algorithms \n",
    "    # Evaluate performances of \"Surprise\" algorithm on the dataset\n",
    "    perf[algo_str[i]] = evaluate(algo, ratings_surpr, measures=['RMSE']) # Evaluate the performance of each algo by cross validation \n",
    "    print_perf(perf[algo_str[i]]) # Print the performance for each algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- SAVE -----------------\n",
    "# Uncomment the line just below if you want to save the variable\n",
    "#np.save('perf_dictionary.npy', perf) # Saving the dictionary that contains the RMSE of all the algos evaluated above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of the algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "- Optimization of the parameters for a given algorithm with given parameters\n",
    "- TODO: IVAN'S code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********** Define the parameters' grid and the grid search ***********\n",
    "param_grid = {'init_mean': [0,2,4], \n",
    "              'init_std_dev': [0.1, 0.3, 0.5],\n",
    "              'lr_all': [0.002, 0.004, 0.006, 0.008, 0.01],\n",
    "              'reg_all': [0.01, 0.03, 0.05, 0.07, 0.1]}\n",
    "\n",
    "grid_search = GridSearch(SVDpp, param_grid, measures=['RMSE', 'FCP'],\n",
    "                         verbose=False)\n",
    "\n",
    "ratings_ = formating_data_surprise(ratings) # Formating the ratings in the correct format for Surprise\n",
    "\n",
    "\n",
    "# First train an SVD algorithm on the \n",
    "ratings_.split(n_folds=3) # Creation of the 3 Folds\n",
    "grid_search.evaluate(ratings_) # Run the grid search on the cross-validation\n",
    "\n",
    "# best RMSE score\n",
    "print(grid_search.best_score['RMSE']) # Will print the lowest RMSE\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(grid_search.best_params['RMSE']) # Will print the parameters that give the best (lowest) RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use algorithms' predictions to find the best predicted ratings\n",
    "\n",
    "First we set up all the variables, algorithms, regressors and classifiers\n",
    "\n",
    "1. Majority vote\n",
    "2. Mean of the predictions\n",
    "3. Apply classifiers on the predictions (from sklearn)\n",
    "4. Apply regressors on the predictions (from sklearn)\n",
    "5. TODO: algorithm Home made??? --> If yes, add it in the \"Structure of the code\"\n",
    "\n",
    "Then we will choose the best one of the five cited just above and try to optimize some parameters to have an even better prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up everything (algorithms, regressors and classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Algos with optimized parameters ##############\n",
    "#SVD(n_factors=100,lr_all=0.001,reg_all=10**(-1.5))\n",
    "#BaselineOnly(bsl_options={'method': 'als', 'reg_u': 14.4, 'reg_i': 0.3})\n",
    "#KNNWithZScore(k=100, min_k=7, sim_options={'name':'pearson_baseline','user_based':False,'shrinkage':500})\n",
    "#KNNBaseline(k=96, min_k=8,sim_options={'name': 'pearson_baseline','user_based': False,'shrinkage': 500},bsl_options={'method': 'als','reg_u': 14.4,'reg_i': 0.3})\n",
    "#NMF(n_factors=35,reg_pu=10**(-1.5),reg_qi=10**(-0.5))\n",
    "# TODO --> put the correct parameters\n",
    "\n",
    "#****** Options ********\n",
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False ,\n",
    "               'shrinkage': 500\n",
    "              }\n",
    "#***** All the algorithms we are using (optimized) *******\n",
    "algos = [SVDpp(n_factors=10,lr_all=0.00177827941004,reg_all=0.001),\n",
    "         KNNBaseline(k=96, min_k=8,sim_options={'name': 'pearson_baseline','user_based': False,'shrinkage': 500},bsl_options={'method': 'als','reg_u': 14.4,'reg_i': 0.3}),\n",
    "         NMF(n_factors=35,reg_pu=10**(-1.5),reg_qi=10**(-0.5)),\n",
    "         SVD(n_factors=100,lr_all=0.001,reg_all=10**(-1.5)),\n",
    "         SlopeOne(),\n",
    "         BaselineOnly(bsl_options={'method': 'als', 'reg_u': 14.4, 'reg_i': 0.3}),\n",
    "         KNNWithZScore(k=100, min_k=7, sim_options={'name':'pearson_baseline','user_based':False,'shrinkage':500})\n",
    "        ]\n",
    "\n",
    "\n",
    "############ Define some variables #################\n",
    "columns_name = ['SVDpp','KNNBaseline','NMF','SVD','SlopeOne','BaselineOnly','KNNWithZScore'] # Name of the algorithms, useful for the dataframe\n",
    "\n",
    "#*********** Regressors ***************\n",
    "regressions_method = [linear_model.LinearRegression(), \n",
    "                      linear_model.Ridge(), \n",
    "                      linear_model.Lasso(), \n",
    "                      linear_model.BayesianRidge(),\n",
    "                      linear_model.ElasticNet(),\n",
    "                      linear_model.HuberRegressor(),\n",
    "                      linear_model.LassoLars(),\n",
    "                      linear_model.PassiveAggressiveRegressor(),\n",
    "                      linear_model.SGDRegressor()\n",
    "                     ]\n",
    "\n",
    "#*********** Classifiers ***************\n",
    "classifiers_method = [naive_bayes,\n",
    "                      kNearestNeigh, \n",
    "                      decision_tree, \n",
    "                      neural_net, \n",
    "                      support_vectorMachine, \n",
    "                      discr_analysis, \n",
    "                      lin_discr_analysis\n",
    "                     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= TRAINING ====================\n",
    "#********* Creation of train and validation ratings *********\n",
    "train_ratings, validation_ratings = split_data(ratings, prob_test=0.3) # splitting in train and validation set\n",
    "\n",
    "#********* Do the training with all the algos ****************\n",
    "print('-----START -----\\nTraining of the algos\\n')\n",
    "algos_trained = first_train(train_ratings,algos) # Call function first_train\n",
    "second_df, moviesID_userID_df = second_train_df(prediction_df, columns_name) # Call function second_train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Validation set up ============\n",
    "#******** prepare the validation set *********\n",
    "validation_df, validation_surprise = formating_data_surprise(validation_ratings, True) # Formating the data in order to use Surprise\n",
    "validation_set = validation_surprise.build_full_trainset() # Build trainset\n",
    "validation_set_pred = validation_set.build_testset() # Build iterable object in order to test \n",
    "\n",
    "prediction_validation_df = validation_df.copy() # Initialization of the DataFrame we will return\n",
    "######### Predictions by the trained algorithms #############\n",
    "for i, algo_t in enumerate(algos_trained): # Loop over all the trained algorithms\n",
    "    pred = algo_t.test(validation_set_pred) # Make the prediction\n",
    "\n",
    "    ########## Creation of the list: estim ########\n",
    "    estim = [] # initialization of the list estim\n",
    "\n",
    "    for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "        estim.append(p.est) # fill this list with the ratings\n",
    "\n",
    "    d = {'prediction' : pd.Series(estim)}\n",
    "    temp = pd.DataFrame(d)\n",
    "    prediction_validation_df = pd.concat([prediction_validation_df,temp], axis=1)\n",
    "first_col = ['movies ID', 'Label', 'users ID']\n",
    "all_col = first_col + columns_name\n",
    "prediction_validation_df.columns = all_col\n",
    "# prediction_validation_df contains \"movies ID\", \"users ID\", \"ratings\" and the \"prediction\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the data we get after the predictions on the validation set')\n",
    "display(prediction_validation_df.head())\n",
    "print('\\nThis is the data we get after the predictions on the validation set and after just removed \"movies ID\" and \"users ID\"')\n",
    "prediction_label_df = prediction_validation_df.copy()\n",
    "prediction_label_df = prediction_label_df.drop(['movies ID', 'users ID'], axis = 1)\n",
    "display(prediction_label_df.head())\n",
    "#----------- SAVE -----------------\n",
    "# Uncomment thes lines just below if you want to save the variable\n",
    "\n",
    "#prediction_validation_df.to_csv('prediction_df_validation.csv')\n",
    "#np.save('algos_trained_training', algos_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Majority vote\n",
    "- We use the algorithms trained on the training set to create predictions on the validation set. Then we apply the majority vote algorithm in order to obtain a final prediction. We also compute the rmse with the real ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************* Apply majority on the validation set *************\n",
    "validation_pred_majority, validation_rmse_majority = majority_vote(prediction_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE obtained on the validation set with Majority vote: {}'.format(validation_rmse_majority))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean vote\n",
    "- We use the algorithms trained on the training set to create predictions on the validation set. Then we apply the mean vote algorithm in order to obtain a final prediction. We also compute the rmse with the real ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************* Apply mean on the validation set *************\n",
    "validation_pred_mean, validation_rmse_mean = mean_vote(prediction_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE obtained on the validation set with Mean vote: {}'.format(validation_rmse_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classifiers on predictions\n",
    "- We use the predictions done by the trained (optimized) algorithms and try to apply several different classifiers on them.\n",
    "- We will choose the best classifier based on the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Find the best classifier (with the lowest RMSE) **********\n",
    "print('-----START -----\\nSelection of classifier\\n')\n",
    "\n",
    "clf, best_rmse_clf = apply_classifier(prediction_label_df, classifiers_method) # Call a method that will find the best classifier and its RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Print the best classifier and its RMSE ############\n",
    "print(clf)\n",
    "print('\\nRMSE obtained with the best classifier: {}'.format(best_rmse_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "\n",
    "RMSE obtained with the best classifier: 1.4029965619933586"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regressors on predictions\n",
    "- We use the predictions done by the trained (optimized) algorithms and try to apply several different regressors on them.\n",
    "- We will choose the best regressor based on the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Find the best classifier (with the lowest RMSE) **********\n",
    "print('-----START -----\\nSelection of regressor\\n')\n",
    "reg, best_rmse_reg = lin_regressors(prediction_label_df, regressions_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Print the best regressor and its RMSE ############\n",
    "print(reg)\n",
    "print('\\nRMSE obtained with the best regressor: {}'.format(best_rmse_reg))\n",
    "print('\\nThe coefficients are: {}'.format(reg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
    "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
    "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
    "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
    "\n",
    "RMSE obtained with the best regressor: 0.982891103210869"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- We can clearly see that the regression technique is the one that works better\n",
    "- So now we will try to improve the best regressor by optimizing the parameters of this classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------- NEW -------------\n",
    "Should be where there is everything done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Train the best classifier and the best regressor on the whole training set **********\n",
    "training_prediction_set, training_prediction_label = get_label_predictions(second_df) \n",
    "\n",
    "clf.fit(training_prediction_set, training_prediction_label)\n",
    "reg.fit(training_prediction_set, training_prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Creation of Submission ##############\n",
    "prediction_test_df = first_train(ratings,algos, test,submit = True) # Training on the whole set  \n",
    "train_df, moviesID_userID_df = second_train_df(prediction_test_df, columns_name) # Get the predictions from the algos\n",
    "\n",
    "onlyPrediction = train_df.drop(train_df.columns[[0]], axis=1)\n",
    "\n",
    "y_prediction_clf = clf.predict(onlyPrediction) # Apply the classifier\n",
    "y_prediction_reg = onlyPrediction.dot(reg.coef_)\n",
    "moviesID_userID_df['Prediction'] = y_prediction_clf # Add a prediction columns\n",
    "\n",
    "######### Create the CSV files ##########\n",
    "name = 'prediction_clf.csv' # Name of the file\n",
    "create_csv_submission(moviesID_userID_df['users ID'], moviesID_userID_df['movies ID'], moviesID_userID_df['Prediction'], name) # To create the CSV file\n",
    "\n",
    "name = 'prediction_reg.csv'\n",
    "moviesID_userID_df['Prediction'] = y_prediction_reg\n",
    "create_csv_submission(moviesID_userID_df['users ID'], moviesID_userID_df['movies ID'], moviesID_userID_df['Prediction'], name) # To create the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "#prediction_test_df.to_csv('prediction_test_df_createSub.csv')\n",
    "#train_df.to_csv('train_df_createSub.csv')\n",
    "#moviesID_userID_df.to_csv('moviesID_userID_df_createSub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- We take the best regressor and now we will try to find the best parameters for this regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO /////////////// DELETE \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "second_df_imp = pd.read_csv('./ALL_SAVE_BEST_RUN/second_df_training.csv')\n",
    "# NEED TO REDUC THE SIZE because of the \"read_csv\" which add a first column...\n",
    "second_df = second_df_imp.copy()\n",
    "second_df = second_df.drop(second_df.columns.values[0], axis=1)\n",
    "col_one_df = pd.DataFrame(np.ones(second_df.shape[0]))\n",
    "second_df = pd.concat([col_one_df, second_df], axis=1)\n",
    "display(second_df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Define the best regressor and some parameters ############\n",
    "regressor_toOptimize = linear_model.SGDRegressor()\n",
    "hyperparams = {'alpha': np.logspace(-6,-2,5) , \n",
    "               'l1_ratio': np.linspace(0,0.6,5),  \n",
    "               'epsilon': np.logspace(-3,1,5)\n",
    "               }\n",
    "#hyperparams = {'epsilon': [1,2,3,4,5,6], 'alpha': [0.0001, 0.001,0.01, 0.1], 'tol': [1e-400,1e-100, 1e-6, 1e-5, 1e-4, 1e-2] }\n",
    "#hyperparams = {'epsilon': [1, 1.35, 1.6], 'alpha': [0.00001, 0.0001, 0.001], 'tol': [1e-04,1e-05, 1e-06] }\n",
    "#hyperparams = {'epsilon': [1, 1.35, 1.6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Do the Grid Search optimization ###########\n",
    "regressor_optimized = optimization_regressor(second_df, regressor_toOptimize, hyperparams)\n",
    "print(regressor_optimized) # will show the best param and also all the possible values of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('These are the best parameters: {}'.format(regressor_optimized.best_params_))\n",
    "\n",
    "# Uncomment if you want to save \n",
    "#np.save('best_regressor_param', regressor_optimized.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP HERE --> because now we have everything we need to define our final model and do a prediction\n",
    "\n",
    "## Here I think we can just call \"run()\" and it is done!\n",
    "\n",
    "### Then we need to define how we want to put all the \"optimization\" steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Call our final model \n",
    "from run import *\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_2 import *\n",
    "run_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Load the data #############\n",
    "print('Loading the data')\n",
    "ratings = load_data('./data_train.csv') # Load the training set\n",
    "test = load_data('./sample_submission.csv') # Load the test set, in order to create the submission file later\n",
    "\n",
    "############# Apply algorithms from Surprise #############\n",
    "print('Initializing the algorithms')\n",
    "#************ Initialize the lists of the algorithms with parameters already optimized ************\n",
    "algorithms = [SVDpp(n_factors=10,lr_all=0.00177827941004,reg_all=0.001),\n",
    "         KNNBaseline(k=96, min_k=8,sim_options={'name': 'pearson_baseline','user_based': False,'shrinkage': 500},\n",
    "                     bsl_options={'method': 'als','reg_u': 14.4,'reg_i': 0.3}),\n",
    "         NMF(n_factors=35,reg_pu=10**(-1.5),reg_qi=10**(-0.5)),\n",
    "         SVD(n_factors=100,lr_all=0.001,reg_all=10**(-1.5)),\n",
    "         SlopeOne(),\n",
    "         BaselineOnly(bsl_options={'method': 'als', 'reg_u': 14.4, 'reg_i': 0.3}),\n",
    "         KNNWithZScore(k=100, min_k=7, sim_options={'name':'pearson_baseline','user_based':False,'shrinkage':500})\n",
    "         ]\n",
    "\n",
    "columns_name = ['SVDpp', # This list is usefull to define the name of the columns of the predictions\n",
    "                'KNNBaseline',\n",
    "                'NMF',\n",
    "                'SVD',\n",
    "                'SlopeOne',\n",
    "                'BaselineOnly',\n",
    "                'KNNWithZScore'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********** TRAINING **************\n",
    "\n",
    "#----------- Splitting ------------\n",
    "train_algo_ratings, train_reg_ratings = split_data(ratings, prob_test=0.3) # splitting in train and test set\n",
    "#----------- Training the algorithms on \"train_algo_ratings\" and apply them on \"train_reg_ratings\" ---------------\n",
    "print('Training of the algorithms')\n",
    "algos_trained = first_train(train_algo_ratings, algorithms) # Trained the algorithms on the \"train_algo_ratings\" set\n",
    "#algos_trained = np.load('algos_trained_training.npy')\n",
    "#******** prepare the validation set *********\n",
    "print('---- Start the predictions -----')\n",
    "train_reg_df, train_reg_surprise = formating_data_surprise(train_reg_ratings, True) # Formating the data in order to use Surprise\n",
    "train_reg_set = train_reg_surprise.build_full_trainset() # Build trainset\n",
    "reg_set_pred = train_reg_set.build_testset() # Build iterable object in order to test\n",
    "\n",
    "prediction_reg_df = train_reg_df.copy() # Initialization of the DataFrame we will return\n",
    "######### Predictions by the trained algorithms #############\n",
    "for i, algo_t in enumerate(algos_trained): # Loop over all the trained algorithms\n",
    "    pred = algo_t.test(reg_set_pred) # Make the prediction\n",
    "\n",
    "    ########## Creation of the list: estim ########\n",
    "    estim = [] # initialization of the list estim\n",
    "\n",
    "    for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "        estim.append(p.est) # fill this list with the ratings\n",
    "\n",
    "    d = {'prediction' : pd.Series(estim)}\n",
    "    temp = pd.DataFrame(d)\n",
    "    prediction_reg_df = pd.concat([prediction_reg_df,temp], axis=1)\n",
    "first_col = ['movies ID', 'Label', 'users ID'] # In order to put the right name on the columns\n",
    "all_col = first_col + columns_name # In order to put the right name on the columns\n",
    "prediction_reg_df.columns = all_col # In order to put the right name on the columns\n",
    "print('---- End of the predictions -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Training the regressor on the predictions on the \"train_reg_ratings\" ---------------\n",
    "\n",
    "prediction_reg_cleaned = prediction_reg_df.copy() # Copy the original data, keep it intact\n",
    "prediction_reg_cleaned = prediction_reg_cleaned.drop(['movies ID', 'users ID'], axis = 1) # Remove the columns we don't need\n",
    "\n",
    "print('Apply the regressor')\n",
    "regressor =  linear_model.SGDRegressor(alpha = 0.0001, epsilon= 0.01, l1_ratio= 0.3) # This is the best regressor we've found and optimized\n",
    "#regressor = linear_model.HuberRegressor()\n",
    "training_predictions_set, training_predictions_label = get_label_predictions(prediction_reg_cleaned) # Take the predictions and the labels\n",
    "\n",
    "#___________ Adding the offset parameter (column of 1) __________________\n",
    "col_one = pd.DataFrame(np.ones(training_predictions_set.shape[0])) # Create a column of ones (offset parameter)\n",
    "training_predictions_set= pd.DataFrame(training_predictions_set) # Put the training_prediction_set in Dataframe type\n",
    "training_predictions_set = pd.concat([col_one, training_predictions_set], axis=1) # Add the column of 1, the offset at the prediction set\n",
    "\n",
    "#___________ Training of the regressor _______________\n",
    "regressor.fit(training_predictions_set, training_predictions_label) # Here we do the training, find the weights of the regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Predict the unknown ratings #########\n",
    "print('------ Start the predictions on the unknown --------')\n",
    "#*********** Prepare the test set ***************\n",
    "test_df, test_surprise = formating_data_surprise(test, True) #Put the data in the correct format\n",
    "test_set = test_surprise.build_full_trainset() # Build trainset\n",
    "test_set_pred = test_set.build_testset() # Build iterable object in order to test\n",
    "print('\\tApply the algorithms')\n",
    "prediction_test_df = test_df.copy() # Initialization of the DataFrame we will return\n",
    "#*********** Prediction **************\n",
    "for i, algo_t in enumerate(algos_trained): # Loop over all the trained algorithms\n",
    "    pred = algo_t.test(test_set_pred) # Make the prediction\n",
    "\n",
    "    #_________ Creation of the list: estim __________\n",
    "    estim = [] # initialization of the list estim\n",
    "\n",
    "    for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "        estim.append(p.est) # fill this list with the ratings\n",
    "\n",
    "    d = {'prediction' : pd.Series(estim)}\n",
    "    temp = pd.DataFrame(d)\n",
    "    prediction_test_df = pd.concat([prediction_test_df,temp], axis=1)\n",
    "first_col = ['movies ID', 'Label', 'users ID'] # In order to put the right name on the columns\n",
    "all_col = first_col + columns_name # In order to put the right name on the columns\n",
    "prediction_test_df.columns = all_col # In order to put the right name on the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#___________ Remove the not wanted columns _______________\n",
    "predictions_only = prediction_test_df.copy() # Copy in order to not act on the original one\n",
    "predictions_only = predictions_only.drop(['movies ID','users ID', 'Label'], axis = 1) # remove the \"label\", \"movies ID\" and \"users ID\" column, keep only the predictions\n",
    "\n",
    "#___________ Adding the offset parameter (column of 1) __________________\n",
    "col_one_unknown = pd.DataFrame(np.ones(predictions_only.shape[0])) # Create a column of ones (offset parameter)\n",
    "predictions_only = pd.concat([col_one_unknown, predictions_only], axis=1) # Add the column of 1, the offset at the prediction set\n",
    "\n",
    "#----------- Apply regression on the predictions of the unknown ratings ------------------\n",
    "print('\\tApply the regressor')\n",
    "moviesID_usersID_prediction = prediction_test_df[['movies ID','users ID']]\n",
    "\n",
    "\n",
    "predicted = predictions_only.dot(regressor.coef_) # Compute the predictions of the unknown ratings\n",
    "moviesID_usersID_prediction['Prediction'] = predicted # Now the variable \"movies_usersID_df\" contains all the values we need to create the submission file\n",
    "\n",
    "############ Creation of the submission file #############\n",
    "print('Create the submission file')\n",
    "name = 'best_submission.csv'\n",
    "create_csv_submission(moviesID_usersID_prediction['users ID'], moviesID_usersID_prediction['movies ID'], moviesID_usersID_prediction['Prediction'], name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_df.to_csv('prediction_test_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Use the optimized regressor ###########\n",
    "#reg_opt = linear_model.HuberRegressor(alpha=0.0001, epsilon=1.5, tol=1e-05)\n",
    "reg_opt = linear_model.HuberRegressor(alpha=0.01, epsilon=5, tol=1e-400)\n",
    "print(reg_opt)\n",
    "\n",
    "training_prediction_set, training_prediction_label = get_label_predictions(second_df)\n",
    "col_one_np = np.ones(training_prediction_set.shape[0])\n",
    "\n",
    "col_one = pd.DataFrame(col_one_np)\n",
    "training_prediction_set= pd.DataFrame(training_prediction_set)\n",
    "training_prediction_set = pd.concat([col_one, training_prediction_set], axis=1)\n",
    "print(training_prediction_set.head())\n",
    "reg_opt.fit(training_prediction_set, training_prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_opt.coef_)\n",
    "print(training_prediction_set.shape)\n",
    "print(training_prediction_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Creation of Submission ##############\n",
    "prediction_test_df = first_train(ratings,algos, test,submit = True) # Training on the whole set  \n",
    "train_df, moviesID_userID_df = second_train_df(prediction_test_df, columns_name) # Get the predictions from the algos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyPrediction = train_df.drop(train_df.columns[[0]], axis=1)\n",
    "col_one_sub = pd.DataFrame(np.ones(onlyPrediction.shape[0]))\n",
    "onlyPrediction = pd.concat([col_one_sub, onlyPrediction], axis = 1)\n",
    "print(onlyPrediction.head())\n",
    "print(onlyPrediction[[0]])\n",
    "\n",
    "y_prediction_reg = onlyPrediction.dot(reg_opt.coef_)\n",
    "\n",
    "name = 'prediction_regOPTIMIZED_final_with1.csv'\n",
    "moviesID_userID_df['Prediction'] = y_prediction_reg\n",
    "create_csv_submission(moviesID_userID_df['users ID'], moviesID_userID_df['movies ID'], moviesID_userID_df['Prediction'], name) # To create the CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------- END of new -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Define: algo, dataset (trainset ##############\n",
    "ratings_ = formating_data_surprise(ratings)\n",
    "trainset = ratings_.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Define: testset ##############\n",
    "dataF_test_ratings_, test_ratings_ = formating_data_surprise(test, True)\n",
    "test_trainset = test_ratings_.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = test_trainset.build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## STEF ADD ###################\n",
    "########## Train and test the algo ###########\n",
    "algorithm = [SVDpp(),KNNBaseline(),NMF(),SVD(),SlopeOne(),BaselineOnly(),KNNWithZScore()]\n",
    "\n",
    "prediction_df = dataF_test_ratings_.copy()\n",
    "for i, algo in enumerate (algorithm):\n",
    "    \n",
    "    algo.train(trainset) # Training of the algo\n",
    "    pred = algo.test(testset) # Make the prediction\n",
    "\n",
    "    ########## Creation of the lists: row_users, col_movies, estim ########\n",
    "    estim = [] # initialization of the list estim\n",
    "\n",
    "    for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "        estim.append(p.est) # fill this list with the ratings\n",
    "    \n",
    "    d = {'prediction' : pd.Series(estim)}\n",
    "    temp = pd.DataFrame(d)\n",
    "    prediction_df = pd.concat([prediction_df,temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### STEF ADD ################\n",
    "######### Create the submission #########\n",
    "#******** Take the prediction of the algos ********\n",
    "prediction_df_clean = prediction_df.copy() # copy in order to not modify the original \n",
    "prediction_df_clean = prediction_df_clean.drop(prediction_df_clean.columns.values[0:3], axis = 1) # remove the columns that we don't want\n",
    "\n",
    "#******** Apply the classifier ***********\n",
    "prediction_clas = clf.predict(prediction_df_clean) # Apply the classifier on the predictions\n",
    "\n",
    "#******** Prepare the variables for submission *********\n",
    "usersID = prediction_df['users ID'] # To have the user ID\n",
    "moviesID = prediction_df['movies ID'] # To have the movies ID\n",
    "name = 'all_algos_SVM_noOptimization.csv' # The name of the csv file\n",
    "create_csv_submission(usersID, moviesID, prediction_clas, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Train and test the algo ###########\n",
    "\n",
    "#n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.2\n",
    "\n",
    "#algorithm = SVD(n_epochs = 20, lr_all = 0.002, reg_all = 0.2)\n",
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False ,\n",
    "               'shrinkage': 500\n",
    "              }\n",
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "algorithm_sim.train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = algorithm_sim.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Creation of the lists: row_users, col_movies, estim ########\n",
    "row_users = [] # initialization of the list row_users\n",
    "col_movies = [] # initialization of the list col_movies\n",
    "estim = [] # initialization of the list estim\n",
    "for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "    row_users.append(p.uid) # fill this list with the indices of the users\n",
    "    col_movies.append(p.iid) # fill this list with the indices of the movies\n",
    "    estim.append(p.est) # fill this list with the ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Create the CSV files ##########\n",
    "name = 'KNNBaseline.csv' # Name of the file\n",
    "\n",
    "#estim = only_prediction_df['Majority'] # This is only here in order to use the estimation done previously\n",
    "\n",
    "#estim = prediction_df['prediction'].mean(axis = 1)\n",
    "#print(estim.shape)\n",
    "create_csv_submission(row_users, col_movies, estim, name) # To create the CSV file \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests with different similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Define: algo, dataset (trainset ##############\n",
    "ratings_bsl = formating_data_surprise(ratings)\n",
    "#trainset_bsl = ratings_bsl.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_bsl.split(n_folds=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm1 = BaselineOnly()\n",
    "\n",
    "for trainset1, testset1 in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm1.train(trainset1)\n",
    "    predictions1 = algorithm1.test(testset1)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse1 = accuracy.rmse(predictions1, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = evaluate(algorithm1, ratings_bsl, measures=['RMSE'])\n",
    "\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 10,\n",
    "               'reg_u': 10,\n",
    "               'reg_i': 25\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_bsl = BaselineOnly(bsl_options=bsl_options)\n",
    "\n",
    "\n",
    "for trainset1, testset1 in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_bsl.train(trainset1)\n",
    "    predictions1 = algorithm_bsl.test(testset1)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_bsl = accuracy.rmse(predictions1, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = evaluate(algorithm_bsl, ratings_bsl, measures=['RMSE'])\n",
    "\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LONG\n",
    "\n",
    "algorithm2 = KNNBaseline\n",
    "\n",
    "for trainset2, testset2 in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm2.train(trainset2)\n",
    "    predictions2 = algorithm2.test(testset2)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse2 = accuracy.rmse(predictions2, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'user_based': False\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_item = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': True ,\n",
    "               'shrinkage': 0\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LONG\n",
    "\n",
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'user_based': False\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears_user = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False ,\n",
    "               'shrinkage': 500\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears_user = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears_user = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson',\n",
    "               'user_based': False\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': False\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
