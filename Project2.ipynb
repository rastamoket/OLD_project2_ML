{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # Used for the conversion of \"r##_c##\" in only the numbers --> TODO: check where it comes from\n",
    "from IPython.display import display\n",
    "from helpers import *\n",
    "from play_with_data import *\n",
    "from pre_processing import *\n",
    "from matrix_factorization import *\n",
    "from cross_validation import *\n",
    "from apply_classifiers import *\n",
    "from trainings_submissions import *\n",
    "from regressions_models import *\n",
    "import scipy.sparse as sp # In order to use sparse \n",
    "# Predictors imported in performance order (best to worst, according to http://surpriselib.com/)\n",
    "from surprise import SVDpp\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import SlopeOne\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF\n",
    "from surprise import CoClustering\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithZScore # not scored --> to be tested quickly\n",
    "from surprise import dataset\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise import GridSearch\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "\n",
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Creation of a sparse matrix of the data **********\n",
    "ratings = load_data('./data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done before the exercise\n",
    "- This will create a numpy matrix from the data given \"data_train.csv\"\n",
    "- Then we will use this to show some statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Load the given data **********\n",
    "r_c, x = load_data_old('./data_train.csv') #r_c contain the position (user, movie) and x contain the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Creation of a matrix of the data ********\n",
    "nUser = 10000\n",
    "nItem = 1000\n",
    "data = np.zeros([nUser, nItem]) # These numbers were given\n",
    "\n",
    "for ind, i in enumerate(r_c): # Loop over all the ID, in order to create a numpy matrix\n",
    "    data[int(re.findall('\\d+', i)[0])-1, int(re.findall('\\d+', i)[1])-1] = x[ind] # Use the information in the ID (row, col) to create the matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********** Data preview ************\n",
    "# Check if there is any missed data \n",
    "# It was told us that we have the data from 10'000 users for 1000 films, but we don't have all these data\n",
    "info_general(nUser, nItem, x, data)\n",
    "print('\\n')\n",
    "info_ratings(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this barplot, we can see that ratings are not distributed in an uniform way, this may suggest that there is a bias in the rating matrix that has to be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done after the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***** Data preview *********\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings) # Original code is from the course, ex10 'plots.py'\n",
    "print(\"Maximum number of items per user:\\t{}\\nMinimum number of items per user:\\t{}\\n\".format(np.max(num_items_per_user), np.min(num_items_per_user)))\n",
    "print(\"Maximum number of users per item:\\t{}\\nMinimum number of users per item:\\t{}\".format(np.max(num_users_per_item), np.min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done so far\n",
    "- Preview of the data\n",
    "- Pre-processing:\n",
    "    - Choosing only the \"valid ratings\", the users and items that contains more than min_num_ratings\n",
    "    - Splitting the data in test and train, by choosing 90% of the ratings from the valid_ratings and only the non-zeros values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_surpr = formating_data_surprise(ratings)\n",
    "ratings_surpr.split(n_folds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Evaluate all the algorithms ########################\n",
    "algos = [SVDpp(),KNNBaseline(),SVD(),SlopeOne(),BaselineOnly(),KNNWithZScore()]\n",
    "perf = {}\n",
    "algo_str = ['SVDpp', 'KNN Baseline','SVD', 'Slope One', 'BaselineOnly', 'KNN with Z score']\n",
    "\n",
    "for i,algo in enumerate(algos): #for algo in algos:\n",
    "    # Evaluate performances of our algorithm on the dataset.\n",
    "    perf[algo_str[i]] = evaluate(algo, ratings_surpr, measures=['RMSE'])\n",
    "    print_perf(perf[algo_str[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('perf_dictionary.npy', perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'init_mean': [0,2,4], \n",
    "              'init_std_dev': [0.1, 0.3, 0.5],\n",
    "              'lr_all': [0.002, 0.004, 0.006, 0.008, 0.01],\n",
    "              'reg_all': [0.01, 0.03, 0.05, 0.07, 0.1]}\n",
    "\n",
    "grid_search = GridSearch(SVDpp, param_grid, measures=['RMSE', 'FCP'],\n",
    "                         verbose=False)\n",
    "\n",
    "\n",
    "# First train an SVD algorithm on the movielens dataset.\n",
    "ratings_ = formating_data_surprise(ratings)\n",
    "#trainset = ratings_.build_full_trainset()\n",
    "\n",
    "ratings_.split(n_folds=3)\n",
    "grid_search.evaluate(ratings_)\n",
    "\n",
    "# best RMSE score\n",
    "print(grid_search.best_score['RMSE'])\n",
    "# >>> 0.96117566386\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(grid_search.best_params['RMSE'])\n",
    "# >>> {'reg_all': 0.4, 'lr_all': 0.005, 'n_epochs': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Creation of a sparse matrix of the data ******\n",
    "ratings = load_data('./data_train.csv')\n",
    "train_ratings, test_ratings =split_data(ratings, prob_test=0.15) # splitting in train and test set \n",
    "\n",
    "\n",
    "########### Define: algo, dataset (trainset ##############\n",
    "dataF_train, ratings_train = formating_data_surprise(train_ratings, True) # Create the Dataset for surprise (training set)\n",
    "\n",
    "trainset_algo = ratings_train.build_full_trainset() # Build trainset\n",
    "trainset_pred = trainset_algo.build_testset() # Build iterable object in order to test \n",
    "\n",
    "# C'est notre test set donc utiliser plus tard\n",
    "ratings_test = formating_data_surprise(test_ratings)\n",
    "validationset = ratings_test.build_full_trainset()\n",
    "# -------------------------------------------------\n",
    "\n",
    "########## Train and test the algo ###########\n",
    "algorithm = [SVDpp(),KNNBaseline(),NMF(),SVD(),SlopeOne(),BaselineOnly(),KNNWithZScore()]\n",
    "\n",
    "prediction_df = dataF_train.copy()\n",
    "for i, algo in enumerate (algorithm):\n",
    "    \n",
    "    algo.train(trainset_algo) # Training of the algo\n",
    "    pred = algo.test(trainset_pred) # Make the prediction\n",
    "\n",
    "    ########## Creation of the lists: row_users, col_movies, estim ########\n",
    "    estim = [] # initialization of the list estim\n",
    "\n",
    "    for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "        estim.append(p.est) # fill this list with the ratings\n",
    "    \n",
    "    d = {'prediction' : pd.Series(estim)}\n",
    "    temp = pd.DataFrame(d)\n",
    "    prediction_df = pd.concat([prediction_df,temp], axis=1)\n",
    "\n",
    "display(prediction_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cleaning the dataframe ############\n",
    "second_step_dataset_df = prediction_df.copy() # Copy \n",
    "second_step_dataset_df = second_step_dataset_df.drop(second_step_dataset_df.columns[[0, 2]], axis=1) # In order to keep only the real ratings and then the predictions for all algos\n",
    "second_step_dataset_df.columns = ['Label','SVDpp','KNNBaseline','NMF','SVD','SlopeOne','BaselineOnly', 'KNNWithZScore'] # TODO: should be adapted\n",
    "display(second_step_dataset_df.head()) # In order to give an idea of the values\n",
    "\n",
    "second_step_dataset_df.to_csv('predictions_allAlgos.csv') # ADD BY STEF: Save this as a CSV file, because it is long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Use the second_step_dataset.csv saved ################\n",
    "##### ADD BY STEFAN ###\n",
    "# This should only be run if we don't run Majority\n",
    "\n",
    "second_step_dataset_df_imp = pd.read_csv('predictions_allAlgos.csv')\n",
    "second_step_dataset_df = second_step_dataset_df_imp.copy()\n",
    "\n",
    "second_step_dataset_df = second_step_dataset_df.drop(second_step_dataset_df.columns.values[0], axis = 1)\n",
    "display(second_step_dataset_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD stefan\n",
    "\n",
    "\n",
    "regressions_method = [linear_model.LinearRegression, linear_model.Ridge, linear_model.Lasso, linear_model.BayesianRidge]\n",
    "regressors_find, rmse_find = lin_regressors(second_step_dataset_df, regressions_method)\n",
    "\n",
    "print(regressors_find, rmse_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Apply the classifier (SVM) ###############\n",
    "##### ADD BY STEFAN ###\n",
    "classifiers_method = [naive_bayes, kNearestNeigh, decision_tree, neural_net, support_vectorMachine, discr_analysis, lin_discr_analysis]\n",
    "clf, test_error = apply_classifier(second_step_dataset_df, classifiers_method)\n",
    "\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_error)\n",
    "print(clf)\n",
    "bestCLF = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ////// DELETE \\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "# Cleaning the dataframe\n",
    "only_prediction_df = prediction_df['prediction'] # We only take the predictions of the different algorithms\n",
    "only_prediction_df = only_prediction_df.round() # We round the predictions.\n",
    "only_prediction_df.columns = ['SVDpp','KNNBaseline','NMF','SVD','SlopeOne','BaselineOnly','KNNWithZScore'] # Change on column name\n",
    "only_prediction_df['Majority'] = 0\n",
    "#display(only_prediction_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i ,row in only_prediction_df.iterrows(): # We iterate over all the raws to analyse each prediction and chose the\n",
    "                                             # Majority of the prediction\n",
    "        \n",
    "    row_ = row.as_matrix() # Tranform row in np array\n",
    "    unique, counts = np.unique(row_, return_counts=True) # We get set of ratings and their respective count repetition\n",
    "    index_of_max = np.where(counts == np.max(counts)) # We select the index of the max count\n",
    "    max_ = unique[index_of_max]\n",
    "    if max_.shape[0]>1: # If there are same amount of max count, we chose the one with the highest score as we saw\n",
    "                        # that the rating distribution is shifter upwards there is \n",
    "        max_ = max_[-1]\n",
    "    only_prediction_df.loc[i,'Majority'] = max_\n",
    "\n",
    "display(only_prediction_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_prediction_df.to_csv('only_pred.csv')\n",
    "prediction_df.to_csv('all_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------- NEW -------------\n",
    "Should be where there is everything done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Option for algos ***************\n",
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False ,\n",
    "               'shrinkage': 500\n",
    "              }\n",
    "\n",
    "\n",
    "#********* Define some variables **********\n",
    "columns_name = ['SVDpp','KNNBaseline','NMF','SVD','SlopeOne','BaselineOnly','KNNWithZScore']\n",
    "regressions_method = [linear_model.LinearRegression(), \n",
    "                      linear_model.Ridge(), \n",
    "                      linear_model.Lasso(), \n",
    "                      linear_model.BayesianRidge(),\n",
    "                      linear_model.ElasticNet(),\n",
    "                      linear_model.HuberRegressor(),\n",
    "                      linear_model.LassoLars(),\n",
    "                      linear_model.PassiveAggressiveRegressor(),\n",
    "                      linear_model.SGDRegressor()\n",
    "                     ]\n",
    "\n",
    "classifiers_method = [naive_bayes,\n",
    "                      kNearestNeigh, \n",
    "                      decision_tree, \n",
    "                      neural_net, \n",
    "                      support_vectorMachine, \n",
    "                      discr_analysis, \n",
    "                      lin_discr_analysis\n",
    "                     ]\n",
    "algos = [SVDpp(),\n",
    "         KNNBaseline(sim_options=sim_options),\n",
    "         NMF(),\n",
    "         SVD(),\n",
    "         SlopeOne(),\n",
    "         BaselineOnly(),\n",
    "         KNNWithZScore(sim_options=sim_options)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Training and Validating the model ################3\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= TRAINING ====================\n",
    "#********* Creation of train and validation ratings *********\n",
    "train_ratings, validation_ratings =split_data(ratings, prob_test=0.15) # splitting in train and test set\n",
    "\n",
    "#********* Do the training with all the algos ****************\n",
    "print('-----START -----\\nTraining of the algos\\n')\n",
    "prediction_df, algos_trained = first_train(train_ratings,algos) # Call function first_train\n",
    "second_df, moviesID_userID_df = second_train_df(prediction_df, columns_name) # Call function second_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE \n",
    "prediction_df.to_csv('prediction_df_training.csv')\n",
    "second_df.to_csv('second_df_training.csv')\n",
    "moviesID_userID_df.to_csv('moviesID_userID_df_training.csv')\n",
    "np.save('algos_trained_training', algos_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Find the best classifier (with the lowest test error) **********\n",
    "print('-----START -----\\nSelection of classifier and regressor\\n')\n",
    "clf, best_test_error = apply_classifier(second_df, classifiers_method, ratio=0.80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg, best_rmse = lin_regressors(second_df, regressions_method, ratio = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Train the best classifier and the best regressor on the whole training set **********\n",
    "training_prediction_set, training_prediction_label = get_label_predictions(second_df)\n",
    "clf.fit(training_prediction_set, training_prediction_label)\n",
    "reg.fit(training_prediction_set, training_prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "np.save('clf_training_wholeSet', clf)\n",
    "np.save('reg_training_wholeSet', reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Validating ===================\n",
    "print('-----START -----\\nValidation set\\n')\n",
    "prediction_val_df, _algo = first_train(validation_ratings,algos_trained) # HERE SOMETHING IS NOT CORRECT, we should not train on the validation set\n",
    "second_df_val, moviesID_userID_df = second_train_df(prediction_val_df, columns_name)\n",
    "\n",
    "onlyPREDICTION = second_df_val.drop(second_df_val.columns[[0]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "prediction_val_df.to_csv('prediction_val_df_validation.csv')\n",
    "second_df_val.to_csv('second_df_val_validation.csv')\n",
    "moviesID_userID_df.to_csv('moviesID_userID_df_validation.csv')\n",
    "onlyPREDICTION.to_csv('onlyPREDICTION_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******* End of validation **************\n",
    "print('-----START -----\\nPREDICTION by best classifier and best regressor\\n')\n",
    "y_prediction_clf = clf.predict(onlyPREDICTION)  # Do the classification\n",
    "y_prediction_reg = onlyPREDICTION.dot(reg.coef_)\n",
    "# Here we need to check \n",
    "RMSE_clf = np.sqrt(mean_squared_error(second_df_val['Label'], y_prediction_clf))\n",
    "RMSE_reg = np.sqrt(mean_squared_error(second_df_val['Label'], y_prediction_reg))\n",
    "print('RMSE with classifier:\\t{}\\nRMSE with regressor:\\t{}'.format(RMSE_clf, RMSE_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf)\n",
    "print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Creation of Submission ##############\n",
    "prediction_test_df = first_train(ratings,algos, test,submit = True) # Training on the whole set  \n",
    "train_df, moviesID_userID_df = second_train_df(prediction_test_df, columns_name) # Get the predictions from the algos\n",
    "\n",
    "onlyPrediction = train_df.drop(train_df.columns[[0]], axis=1)\n",
    "\n",
    "y_prediction_clf = clf.predict(onlyPrediction) # Apply the classifier\n",
    "y_prediction_reg = onlyPrediction.dot(reg.coef_)\n",
    "moviesID_userID_df['Prediction'] = y_prediction_clf # Add a prediction columns\n",
    "\n",
    "######### Create the CSV files ##########\n",
    "name = 'prediction_clf.csv' # Name of the file\n",
    "create_csv_submission(moviesID_userID_df['users ID'], moviesID_userID_df['movies ID'], moviesID_userID_df['Prediction'], name) # To create the CSV file\n",
    "\n",
    "name = 'prediction_reg.csv'\n",
    "moviesID_userID_df['Prediction'] = y_prediction_reg\n",
    "create_csv_submission(moviesID_userID_df['users ID'], moviesID_userID_df['movies ID'], moviesID_userID_df['Prediction'], name) # To create the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "prediction_test_df.to_csv('prediction_test_df_createSub.csv')\n",
    "train_df.to_csv('train_df_createSub.csv')\n",
    "moviesID_userID_df.to_csv('moviesID_userID_df_createSub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------- END of new -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Define: algo, dataset (trainset ##############\n",
    "ratings_ = formating_data_surprise(ratings)\n",
    "trainset = ratings_.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Define: testset ##############\n",
    "dataF_test_ratings_, test_ratings_ = formating_data_surprise(test, True)\n",
    "test_trainset = test_ratings_.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = test_trainset.build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## STEF ADD ###################\n",
    "########## Train and test the algo ###########\n",
    "algorithm = [SVDpp(),KNNBaseline(),NMF(),SVD(),SlopeOne(),BaselineOnly(),KNNWithZScore()]\n",
    "\n",
    "prediction_df = dataF_test_ratings_.copy()\n",
    "for i, algo in enumerate (algorithm):\n",
    "    \n",
    "    algo.train(trainset) # Training of the algo\n",
    "    pred = algo.test(testset) # Make the prediction\n",
    "\n",
    "    ########## Creation of the lists: row_users, col_movies, estim ########\n",
    "    estim = [] # initialization of the list estim\n",
    "\n",
    "    for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "        estim.append(p.est) # fill this list with the ratings\n",
    "    \n",
    "    d = {'prediction' : pd.Series(estim)}\n",
    "    temp = pd.DataFrame(d)\n",
    "    prediction_df = pd.concat([prediction_df,temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### STEF ADD ################\n",
    "######### Create the submission #########\n",
    "#******** Take the prediction of the algos ********\n",
    "prediction_df_clean = prediction_df.copy() # copy in order to not modify the original \n",
    "prediction_df_clean = prediction_df_clean.drop(prediction_df_clean.columns.values[0:3], axis = 1) # remove the columns that we don't want\n",
    "\n",
    "#******** Apply the classifier ***********\n",
    "prediction_clas = clf.predict(prediction_df_clean) # Apply the classifier on the predictions\n",
    "\n",
    "#******** Prepare the variables for submission *********\n",
    "usersID = prediction_df['users ID'] # To have the user ID\n",
    "moviesID = prediction_df['movies ID'] # To have the movies ID\n",
    "name = 'all_algos_SVM_noOptimization.csv' # The name of the csv file\n",
    "create_csv_submission(usersID, moviesID, prediction_clas, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Train and test the algo ###########\n",
    "\n",
    "#n_epochs': 20, 'lr_all': 0.002, 'reg_all': 0.2\n",
    "\n",
    "#algorithm = SVD(n_epochs = 20, lr_all = 0.002, reg_all = 0.2)\n",
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False ,\n",
    "               'shrinkage': 500\n",
    "              }\n",
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "algorithm_sim.train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = algorithm_sim.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Creation of the lists: row_users, col_movies, estim ########\n",
    "row_users = [] # initialization of the list row_users\n",
    "col_movies = [] # initialization of the list col_movies\n",
    "estim = [] # initialization of the list estim\n",
    "for p in pred: # To loop over the prediction done by the algo on the test set\n",
    "    row_users.append(p.uid) # fill this list with the indices of the users\n",
    "    col_movies.append(p.iid) # fill this list with the indices of the movies\n",
    "    estim.append(p.est) # fill this list with the ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Create the CSV files ##########\n",
    "name = 'KNNBaseline.csv' # Name of the file\n",
    "\n",
    "#estim = only_prediction_df['Majority'] # This is only here in order to use the estimation done previously\n",
    "\n",
    "#estim = prediction_df['prediction'].mean(axis = 1)\n",
    "#print(estim.shape)\n",
    "create_csv_submission(row_users, col_movies, estim, name) # To create the CSV file \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests with different similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Define: algo, dataset (trainset ##############\n",
    "ratings_bsl = formating_data_surprise(ratings)\n",
    "#trainset_bsl = ratings_bsl.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_bsl.split(n_folds=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm1 = BaselineOnly()\n",
    "\n",
    "for trainset1, testset1 in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm1.train(trainset1)\n",
    "    predictions1 = algorithm1.test(testset1)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse1 = accuracy.rmse(predictions1, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = evaluate(algorithm1, ratings_bsl, measures=['RMSE'])\n",
    "\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 10,\n",
    "               'reg_u': 10,\n",
    "               'reg_i': 25\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_bsl = BaselineOnly(bsl_options=bsl_options)\n",
    "\n",
    "\n",
    "for trainset1, testset1 in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_bsl.train(trainset1)\n",
    "    predictions1 = algorithm_bsl.test(testset1)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_bsl = accuracy.rmse(predictions1, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = evaluate(algorithm_bsl, ratings_bsl, measures=['RMSE'])\n",
    "\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LONG\n",
    "\n",
    "algorithm2 = KNNBaseline\n",
    "\n",
    "for trainset2, testset2 in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm2.train(trainset2)\n",
    "    predictions2 = algorithm2.test(testset2)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse2 = accuracy.rmse(predictions2, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'user_based': False\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_item = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': True ,\n",
    "               'shrinkage': 0\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LONG\n",
    "\n",
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'user_based': False\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears_user = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False ,\n",
    "               'shrinkage': 500\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears_user = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': False \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears_user = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson',\n",
    "               'user_based': False\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': False\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_sim = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "for trainset_sim, testset_sim in ratings_bsl.folds():\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algorithm_sim.train(trainset_sim)\n",
    "    predictions_sim = algorithm_sim.test(testset_sim)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    rmse_sim_pears = accuracy.rmse(predictions_sim, verbose=True)\n",
    "    break\n",
    "\n",
    "# Evaluate performances of our algorithm on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DON'T CARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "   - Il est normal de mettre un threshold (min_num_ratings) parce qu'on veut un certain nombre de données pour ce prononcer quand à donner une note, ceci implique qu'on enlève des users et des items, du coup notre matrice des ratings va être plus petite. Quand on la remplis et on fait un submit, comment est-ce qu'on gére ça?\n",
    "       - Ici je suppose que c'est bien de faire la selection des ratings pour le train et test --> comme ceci on est pas ou moins biaisé par les movies et users qui n'ont que des 0 et qui donc n'apporte rien apart du \"bruit\"\n",
    "       - Pour le remplissage de ce que l'on doit submit j'ai fait une petite comparaison entre les data que l'on nous donne et ce qui se trouve dans le sample_submission, (enfin je vais le faire) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********** Pre-processing **********\n",
    "min_num_ratings = 10 # or 15 this is based on the information given above\n",
    "ratings_valid = valid_ratings(ratings, num_items_per_user, num_users_per_item,min_num_ratings)\n",
    "train, test = split_data(ratings_valid) # This will put 90% of the items for the users that have at least one non-zero entry and the 10% in test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_application(train ,False ,4,0.01, 20, 0.2, 0.3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Try ************\n",
    "#rmse = rmse_movie_mean(train, test)\n",
    "rmse = matrix_factorization_SGD(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
