{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # Used for the conversion of \"r##_c##\" in only the numbers --> TODO: check where it comes from\n",
    "from IPython.display import display\n",
    "from helpers import *\n",
    "from play_with_data import *\n",
    "from pre_processing import *\n",
    "from matrix_factorization import *\n",
    "from cross_validation import *\n",
    "from apply_classifiers import *\n",
    "from trainings_submissions import *\n",
    "from regressions_models import *\n",
    "import scipy.sparse as sp # In order to use sparse \n",
    "# Predictors imported in performance order (best to worst, according to http://surpriselib.com/)\n",
    "from surprise import SVDpp\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import SlopeOne\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF\n",
    "from surprise import CoClustering\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithZScore # not scored --> to be tested quickly\n",
    "from surprise import dataset\n",
    "from surprise import Dataset\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise import GridSearch\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "\n",
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******** Creation of a sparse matrix of the data **********\n",
    "ratings = load_data('./data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Define: algo, dataset (trainset ##############\n",
    "ratings_hyper = formating_data_surprise(ratings)\n",
    "#trainset_hyper = ratings_hyper.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_hyper.split(n_folds=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of SVDpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_n_factors_pp = np.linspace(10,30,3)\n",
    "list_lr_all_pp = np.logspace(-3.5,-2,3)\n",
    "list_reg_all_pp = np.logspace(-3,-1.5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_factors_pp = 0\n",
    "best_lr_all_pp = 0\n",
    "best_reg_all_pp = 0\n",
    "best_rmse_pp = 1000\n",
    "\n",
    "for n_factors in list_n_factors_pp:\n",
    "    for lr_all in list_lr_all_pp:\n",
    "        for reg_all in list_reg_all_pp:\n",
    "            n_factors = int(n_factors)\n",
    "            print(n_factors,lr_all,reg_all)\n",
    "\n",
    "            algorithm_bsl = SVDpp(n_factors=n_factors,lr_all=lr_all,reg_all=reg_all)\n",
    "\n",
    "\n",
    "            for trainset1, testset1 in ratings_hyper.folds():\n",
    "\n",
    "                # train and test algorithm.\n",
    "                algorithm_bsl.train(trainset1)\n",
    "                predictions1 = algorithm_bsl.test(testset1)\n",
    "\n",
    "                # Compute and print Root Mean Squared Error\n",
    "                rmse = accuracy.rmse(predictions1, verbose=True)\n",
    "                break\n",
    "            if rmse < best_rmse_pp:\n",
    "                print(\"Improvement!\")\n",
    "                best_n_factors_pp = n_factors\n",
    "                best_lr_all_pp = lr_all\n",
    "                best_reg_all_pp = reg_all\n",
    "                best_rmse_pp = rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_n_factors_pp,best_lr_all_pp,best_reg_all_pp,best_rmse_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 0.00177827941004 0.001 0.985924781713"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_n_factors_nmf = np.linspace(10,20,3)\n",
    "list_reg_pu = np.logspace(-2.5,-1,3)\n",
    "list_reg_qi = np.logspace(-2.5,-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_factors_nmf = 0\n",
    "best_reg_pu = 0\n",
    "best_reg_qi = 0\n",
    "best_rmse_nmf = 1000\n",
    "\n",
    "for n_factors in list_n_factors_nmf:\n",
    "    for reg_pu in list_reg_pu:\n",
    "        for reg_qi in list_reg_qi:\n",
    "            n_factors = int(n_factors)\n",
    "            print(n_factors,reg_pu,reg_qi)\n",
    "\n",
    "            algorithm_bsl = NMF(n_factors=n_factors,reg_pu=reg_pu,reg_qi=reg_qi)\n",
    "\n",
    "\n",
    "            for trainset1, testset1 in ratings_hyper.folds():\n",
    "\n",
    "                # train and test algorithm.\n",
    "                algorithm_bsl.train(trainset1)\n",
    "                predictions1 = algorithm_bsl.test(testset1)\n",
    "\n",
    "                # Compute and print Root Mean Squared Error\n",
    "                rmse = accuracy.rmse(predictions1, verbose=True)\n",
    "                break\n",
    "            if rmse < best_rmse_nmf:\n",
    "                print(\"Improvement!\")\n",
    "                best_n_factors_nmf = n_factors\n",
    "                best_reg_pu = reg_pu\n",
    "                best_reg_qi = reg_qi\n",
    "                best_rmse_nmf = rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_n_factors_nmf,best_reg_pu,best_reg_qi,best_rmse_nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 0.00177827941004 0.001\n",
    "RMSE: 1.0029"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DON'T CARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "   - Il est normal de mettre un threshold (min_num_ratings) parce qu'on veut un certain nombre de données pour ce prononcer quand à donner une note, ceci implique qu'on enlève des users et des items, du coup notre matrice des ratings va être plus petite. Quand on la remplis et on fait un submit, comment est-ce qu'on gére ça?\n",
    "       - Ici je suppose que c'est bien de faire la selection des ratings pour le train et test --> comme ceci on est pas ou moins biaisé par les movies et users qui n'ont que des 0 et qui donc n'apporte rien apart du \"bruit\"\n",
    "       - Pour le remplissage de ce que l'on doit submit j'ai fait une petite comparaison entre les data que l'on nous donne et ce qui se trouve dans le sample_submission, (enfin je vais le faire) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********** Pre-processing **********\n",
    "min_num_ratings = 10 # or 15 this is based on the information given above\n",
    "ratings_valid = valid_ratings(ratings, num_items_per_user, num_users_per_item,min_num_ratings)\n",
    "train, test = split_data(ratings_valid) # This will put 90% of the items for the users that have at least one non-zero entry and the 10% in test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_application(train ,False ,4,0.01, 20, 0.2, 0.3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Try ************\n",
    "#rmse = rmse_movie_mean(train, test)\n",
    "rmse = matrix_factorization_SGD(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
